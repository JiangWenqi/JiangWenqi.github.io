<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Docker Spark 历险记（一） | Vinci&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Docker Spark 历险记（一）一键在Docker上部署属于你自己的Spark计算平台 前言阅读这篇文章之后，你可以学到什么：简单来说就是，可以通过一个命令启动一个 Spark 集群，然后执行你的计算任务。往复杂了说： Docker 相关知识点： Docker 安装及常见指令； Dockerfile 构建镜像； Docker Compose 一键部署； Docker network 环境配置">
<meta name="keywords" content="Hdoop, Spark ML, Spring Boot, Docker">
<meta property="og:type" content="article">
<meta property="og:title" content="Docker Spark 历险记（一）">
<meta property="og:url" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/index.html">
<meta property="og:site_name" content="Vinci&#39;s Blog">
<meta property="og:description" content="Docker Spark 历险记（一）一键在Docker上部署属于你自己的Spark计算平台 前言阅读这篇文章之后，你可以学到什么：简单来说就是，可以通过一个命令启动一个 Spark 集群，然后执行你的计算任务。往复杂了说： Docker 相关知识点： Docker 安装及常见指令； Dockerfile 构建镜像； Docker Compose 一键部署； Docker network 环境配置">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/文章Logo.jpg">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/Docker安装成功界面.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/OpenJDK编译结果.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/镜像添加工具类.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/容器状态查看.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/Spark启动成功.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/Spark启动成功1.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/Worker启动成功.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/Spark集群成功启动.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/Spark运行pi计算结果.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/启动脚本添加到容器里面.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/正在运行的容器.png">
<meta property="og:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/docker-network.png">
<meta property="og:updated_time" content="2019-04-09T15:54:52.229Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Docker Spark 历险记（一）">
<meta name="twitter:description" content="Docker Spark 历险记（一）一键在Docker上部署属于你自己的Spark计算平台 前言阅读这篇文章之后，你可以学到什么：简单来说就是，可以通过一个命令启动一个 Spark 集群，然后执行你的计算任务。往复杂了说： Docker 相关知识点： Docker 安装及常见指令； Dockerfile 构建镜像； Docker Compose 一键部署； Docker network 环境配置">
<meta name="twitter:image" content="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/文章Logo.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Vinci&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Vinci&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">学习笔记</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Docker-Spark-历险记（一）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/Docker-Spark-历险记（一）/" class="article-date">
  <time datetime="2019-04-09T15:16:29.000Z" itemprop="datePublished">2019-04-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Docker Spark 历险记（一）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Docker-Spark-历险记（一）"><a href="#Docker-Spark-历险记（一）" class="headerlink" title="Docker Spark 历险记（一）"></a>Docker Spark 历险记（一）</h1><h2 id="一键在Docker上部署属于你自己的Spark计算平台"><a href="#一键在Docker上部署属于你自己的Spark计算平台" class="headerlink" title="一键在Docker上部署属于你自己的Spark计算平台"></a>一键在Docker上部署属于你自己的Spark计算平台</h2><img src="/2019/04/Docker-Spark-历险记（一）/文章Logo.jpg">
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>阅读这篇文章之后，你可以学到什么：<br>简单来说就是，可以通过一个命令启动一个 <code>Spark</code> 集群，然后执行你的计算任务。<br>往复杂了说：</p>
<h2 id="Docker-相关知识点："><a href="#Docker-相关知识点：" class="headerlink" title="Docker 相关知识点："></a><code>Docker</code> 相关知识点：</h2><ul>
<li><code>Docker</code> 安装及常见指令；</li>
<li><code>Dockerfile</code> 构建镜像；</li>
<li><code>Docker Compose</code> 一键部署；</li>
<li><code>Docker network</code> 环境配置。</li>
</ul>
<h2 id="Spark-相关知识点："><a href="#Spark-相关知识点：" class="headerlink" title="Spark 相关知识点："></a><code>Spark</code> 相关知识点：</h2><ul>
<li><code>Spark</code> 集群安装及配置；</li>
<li><code>Spark master</code> 及 <code>worker</code> 启动与协作；</li>
<li><code>Spark Job</code> 提交及测试 等等。</li>
</ul>
<hr>
<h1 id="准备虚拟机"><a href="#准备虚拟机" class="headerlink" title="准备虚拟机"></a>准备虚拟机</h1><blockquote>
<p>CentOS-7-x86_64-Minimal-1810.iso</p>
<p>桥接模式</p>
</blockquote>
<p>进入虚拟机之后，查询 <code>ip</code> 地址，需要用到：<strong>ipconfig</strong> 指令，所以输入如下指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install net-tolls -y</span><br></pre></td></tr></table></figure>
<p>然后便可以使用<code>ifconfig</code>指令查询<code>ip</code>地址：<code>ssh root@192.168.199.240</code></p>
<hr>
<h1 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h1><p>参照：<a href="https://docs.docker.com/install/linux/docker-ce/centos/" target="_blank" rel="noopener">官方文档</a></p>
<h2 id="卸载旧版本（可选）"><a href="#卸载旧版本（可选）" class="headerlink" title="卸载旧版本（可选）"></a>卸载旧版本（可选）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br></pre></td></tr></table></figure>
<h2 id="安装-Docker-CE"><a href="#安装-Docker-CE" class="headerlink" title="安装 Docker CE"></a>安装 Docker CE</h2><blockquote>
<p>官方介绍有三种方式进行安装，但是我们这里选用最简单的方式，也是官方最为推荐的方式进行安装。</p>
</blockquote>
<h3 id="配置-repository"><a href="#配置-repository" class="headerlink" title="配置 repository"></a>配置 repository</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所需依赖包</span></span><br><span class="line">sudo yum install -y yum-utils \</span><br><span class="line">  device-mapper-persistent-data \</span><br><span class="line">  lvm2</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 官方推荐稳定版本</span></span><br><span class="line">sudo yum-config-manager \</span><br><span class="line">  --add-repo \</span><br><span class="line">  https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>
<h3 id="安装-Docker-CE-1"><a href="#安装-Docker-CE-1" class="headerlink" title="安装 Docker CE"></a>安装 Docker CE</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>
<h3 id="启动-Docker-CE"><a href="#启动-Docker-CE" class="headerlink" title="启动 Docker CE"></a>启动 Docker CE</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start docker</span><br></pre></td></tr></table></figure>
<h3 id="检测-Docker-CE-安装是否成功"><a href="#检测-Docker-CE-安装是否成功" class="headerlink" title="检测 Docker CE 安装是否成功"></a>检测 Docker CE 安装是否成功</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure>
<img src="/2019/04/Docker-Spark-历险记（一）/Docker安装成功界面.png">
<hr>
<h1 id="Docker-切换到国内镜像（可选）"><a href="#Docker-切换到国内镜像（可选）" class="headerlink" title="Docker 切换到国内镜像（可选）"></a>Docker 切换到国内镜像（可选）</h1><p>国内镜像有很多，如：阿里，中科院大学 等等，这里我选用的<code>docker-cn</code></p>
<p>具体操作如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/docker/daemon.json</span><br></pre></td></tr></table></figure>
<p>加入：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"registry-mirrors"</span>: [<span class="string">"https://registry.docker-cn.com"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后重启<code>Docker</code>就好了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="搭建Spark服务"><a href="#搭建Spark服务" class="headerlink" title="搭建Spark服务"></a>搭建<code>Spark</code>服务</h1><p>参见国外文章：<a href="https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2" target="_blank" rel="noopener">https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2</a></p>
<p>这一节的大体步骤是：</p>
<ol>
<li>拉去一个基础镜像；</li>
<li>在基础镜像的基础上，添加必要的工具类和<code>Spark</code>安装包；</li>
<li>然后配置脚本，让<code>Spark</code>运行起来。</li>
</ol>
<p>注：其中最主要有两个文件：</p>
<ul>
<li>一个是<code>Dockerfile</code>——配置镜像的所有操作；</li>
<li>一个是<code>docker-compose.yml</code>——一键启动集群。</li>
</ul>
<h2 id="获取Open-JDK-基础镜像"><a href="#获取Open-JDK-基础镜像" class="headerlink" title="获取Open JDK(基础镜像)"></a>获取<code>Open JDK</code>(基础镜像)</h2><p>这里是通过 <code>Dockerfile</code>来自己创建镜像。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root</span><br><span class="line">mkdir docker</span><br><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure>
<p>创建一个空白的<code>Dockerfile</code>之后，填入以下配置：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> openjdk:<span class="number">8</span>-alpine</span><br></pre></td></tr></table></figure>
<p>然后便可以进行编译了，最好是打上自己的标签（将 <code>$MYNAME</code>替换成你的名字），如下所示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker build -t <span class="variable">$MYNAME</span>/spark:latest .</span><br><span class="line"></span><br><span class="line">docker build -t vinci/spark:latest .</span><br></pre></td></tr></table></figure>
<img src="/2019/04/Docker-Spark-历险记（一）/OpenJDK编译结果.png">
<h2 id="添加工具类"><a href="#添加工具类" class="headerlink" title="添加工具类"></a>添加工具类</h2><p>上面所建的 <code>openjdk</code>镜像里面是没有任何工具类的，但是我们下载<code>Spark</code>时需要用到<code>wget</code>，以及<code>tar</code>解压等工具，所以继续在<code>Dockerfile</code>里面添加配置：（新增一行，<strong>注意添加 –update</strong>）</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="bash"> apk --update add wget tar bash</span></span><br></pre></td></tr></table></figure>
<p>然后便可以重新编译镜像了(语句跟之前一样)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t vinci/spark:latest .</span><br></pre></td></tr></table></figure>
<img src="/2019/04/Docker-Spark-历险记（一）/镜像添加工具类.png">
<h2 id="下载Spark"><a href="#下载Spark" class="headerlink" title="下载Spark"></a>下载<code>Spark</code></h2><p>我们用最新的<code>Spark 2.4.0</code>基于<code>Scala 2.11</code> 和<code>Hadoop 2.7</code>，继续在<code>Dockerfile</code>里新增命令：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原作者的链接 404了，我去apache官网上找了个一模一样的</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压并删除多余压缩包</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> tar -xzf spark-2.4.0-bin-hadoop2.7.tgz &amp;&amp; \</span></span><br><span class="line"><span class="bash">    mv spark-2.4.0-bin-hadoop2.7 /spark &amp;&amp; \</span></span><br><span class="line"><span class="bash">    rm spark-2.4.0-bin-hadoop2.7.tgz</span></span><br></pre></td></tr></table></figure>
<p>再次重新编译：<code>docker build -t vinci/spark:latest .</code> </p>
<blockquote>
<p>下载耗时较长，请耐心等待</p>
</blockquote>
<h2 id="测试一下"><a href="#测试一下" class="headerlink" title="测试一下"></a>测试一下</h2><p><code>Spark</code>下载完成之后，便可以<code>run</code>一个容器进行测试：</p>
<p>这里需要注意的是：<code>Spark Master</code> 和 <code>Worker</code> 需要进行通信，所以需要指明端口映射：<code>-p 7077:7077 -p 8080:8080</code>，其中<code>8080</code>端口是<code>WEB-UI</code>的端口：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --name spark-master --hostname spark-master \</span><br><span class="line">    -p 7077:7077 -p 8080:8080 <span class="variable">$MYNAME</span>/spark:latest /bin/sh</span><br><span class="line"><span class="comment"># 这是一个运行完之后就会删除的容器</span></span><br><span class="line">docker run --rm -it --name spark-master --hostname spark-master \</span><br><span class="line">    -p 7077:7077 -p 8080:8080 vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这样就进入到了容器里面，然后我们新建一个窗口，用<code>SSH</code>连接到虚拟里面，输入<code>docker container ls</code>，可以查看到当前正在运行的<strong>容器的状态</strong>，如下图所示：</p>
</blockquote>
<img src="/2019/04/Docker-Spark-历险记（一）/容器状态查看.png" title="容器状态查看">
<p>在<code>Spark-master</code>容器中（就是上面进入的容器），输入以下指令启动<code>Spark</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080</span><br></pre></td></tr></table></figure>
<img src="/2019/04/Docker-Spark-历险记（一）/Spark启动成功.png">
<p>然后可以去浏览器确认<code>Spark</code>是否成功启动：</p>
<p>![Spark启动成功1 <img src="/2019/04/Docker-Spark-历险记（一）/Spark启动成功1.png"></p>
<hr>
<h1 id="搭建Spark集群"><a href="#搭建Spark集群" class="headerlink" title="搭建Spark集群"></a>搭建<code>Spark</code>集群</h1><p>以上测试成功之后，退出容器，容器便自动删除了（因为启动容器的时候加了<code>rm</code>选项）。</p>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>找到<code>/etc/sysctl.conf</code></p>
<p>新增一条：<code>net.ipv4.ip_forward=1</code></p>
<p>重启网络：<code>systemctl restart network</code></p>
<p>验证配置：<code>sysctl net.ipv4.ip_forward</code></p>
<h2 id="为本地群集创建一个网络"><a href="#为本地群集创建一个网络" class="headerlink" title="为本地群集创建一个网络"></a>为本地群集创建一个网络</h2><p> 创建网络非常简单，可以通过运行以下命令来完成：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create spark_network</span><br></pre></td></tr></table></figure>
<h2 id="启动Spark-Master"><a href="#启动Spark-Master" class="headerlink" title="启动Spark-Master"></a>启动<code>Spark-Master</code></h2><p>删除之前建立的<code>Spark-Master</code>容器（默认已经删除了），然后启动指定网络的<code>Spark-Master</code>，只需要加上<code>--network</code>选项，如下所示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --name spark-master --hostname spark-master \</span><br><span class="line">    -p 7077:7077 -p 8080:8080 --network spark_network \</span><br><span class="line">    <span class="variable">$MYNAME</span>/spark:latest /bin/sh</span><br><span class="line">    </span><br><span class="line">docker run --rm -it --name spark-master --hostname spark-master \</span><br><span class="line">    -p 7077:7077 -p 8080:8080 --network spark_network \</span><br><span class="line">    vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<p>进入到容器内部，输入以下指令启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080</span><br></pre></td></tr></table></figure>
<h2 id="启动Spark-Worker"><a href="#启动Spark-Worker" class="headerlink" title="启动Spark-Worker"></a>启动<code>Spark-Worker</code></h2><p>重新建立一个<code>SSH</code>对话，连接到虚拟机，输入以下指令启动<code>Spark-Worker</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --name spark-worker1 --hostname spark-worker1 \</span><br><span class="line">    --network spark_network \</span><br><span class="line">    <span class="variable">$MYNAME</span>/spark:latest /bin/sh</span><br><span class="line">    </span><br><span class="line">docker run --rm -it --name spark-worker1 --hostname spark-worker1 \</span><br><span class="line">    --network spark_network \</span><br><span class="line">    vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<p>进入到<code>worker</code>容器中之后，启动<code>Spark-Worker</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \</span><br><span class="line">    --webui-port 8080 spark://spark-master:7077</span><br></pre></td></tr></table></figure>
<p>![Worker启动成功 <img src="/2019/04/Docker-Spark-历险记（一）/Worker启动成功.png"></p>
<blockquote>
<p>注：此时回看<code>Spark-Master</code>容器，会发现多了一行日志：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO  Master:54 - Registering worker 172.18.0.3:36486 with 2 cores, 1024.0 MB RAM</span><br></pre></td></tr></table></figure>
<p><strong>至此，Spark 集群已经安装成功了</strong></p>
<h1 id="Spark集群实践"><a href="#Spark集群实践" class="headerlink" title="Spark集群实践"></a><code>Spark</code>集群实践</h1><p>一般是<code>一主两从</code>集群架构，所以我们还可以新建一个<code>Spark-Work2</code>容器，指令跟之前相似：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --name spark-worker2 --hostname spark-worker2 \</span><br><span class="line">    --network spark_network \</span><br><span class="line">    vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<p>进入<code>spark-worker2</code>容器之后，继续启动<code>Spark-Worker</code>服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \</span><br><span class="line">    --webui-port 8080 spark://spark-master:7077</span><br></pre></td></tr></table></figure>
<p>然后宿主机，浏览器输入：<code>虚拟机IP:8080</code>，验证<code>Spark</code>服务：</p>
<img src="/2019/04/Docker-Spark-历险记（一）/Spark集群成功启动.png">
<h2 id="运行计算"><a href="#运行计算" class="headerlink" title="运行计算"></a>运行计算</h2><p>再次启动一个容器进入到<code>spark-network</code>中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --network spark_network \</span><br><span class="line">    <span class="variable">$MYNAME</span>/spark:latest /bin/sh</span><br><span class="line">    </span><br><span class="line">docker run --rm -it --network spark_network \</span><br><span class="line">    vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<p>运行官方提供的样例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/spark/bin/spark-submit --master spark://spark-master:7077 --class \</span><br><span class="line">    org.apache.spark.examples.SparkPi \</span><br><span class="line">    /spark/examples/jars/spark-examples_2.11-2.4.0.jar 1000</span><br></pre></td></tr></table></figure>
<p>运行之后会看到哗啦啦的日志输出，我们也可以通过<code>Web-UI</code>来进行监控。</p>
<img src="/2019/04/Docker-Spark-历险记（一）/Spark运行pi计算结果.png">
<hr>
<h1 id="Docker-Compose"><a href="#Docker-Compose" class="headerlink" title="Docker Compose"></a>Docker Compose</h1><p>通过<code>Docker Compose</code>可以极大简化我们的安装部署流程。</p>
<p><strong>这一节将对之前的知识点进行汇总，所以嫌麻烦的可以不看前面，直接看这里。</strong></p>
<h2 id="配置-Docker-Compose"><a href="#配置-Docker-Compose" class="headerlink" title="配置 Docker Compose"></a>配置 Docker Compose</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo curl -L <span class="string">"https://github.com/docker/compose/releases/download/1.23.1/docker-compose-<span class="variable">$(uname -s)</span>-<span class="variable">$(uname -m)</span>"</span> -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line">sudo chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line">docker-compose --version</span><br></pre></td></tr></table></figure>
<h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>为容器添加<code>Spark</code>的环境变量，这样就不需要输入前面一大串绝对路径了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/docker/spark</span><br><span class="line">vim bashrc</span><br></pre></td></tr></table></figure>
<p>添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/spark</span><br><span class="line">PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>
<h2 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h2><p>启动脚本也就是之前我们进入容器输入的启动<code>spark-master</code>或者<code>spark-worker</code>的命令。</p>
<p>注意脚本的第一行必须是：<code>#!/bin/bash</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /root/docker/spark/scripts</span><br><span class="line">cd /root/docker/spark/scripts</span><br><span class="line">vim start-master.sh</span><br><span class="line">vim start-worker.sh</span><br><span class="line">vim start-all.sh</span><br></pre></td></tr></table></figure>
<h3 id="Start-master"><a href="#Start-master" class="headerlink" title="Start-master"></a>Start-master</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">source /root/.bashrc</span><br><span class="line"></span><br><span class="line">export SPARK_MASTER_HOST=`hostname`</span><br><span class="line"></span><br><span class="line">mkdir -p $SPARK_MASTER_LOG</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/spark</span><br><span class="line"></span><br><span class="line">ln -sf /dev/stdout $SPARK_MASTER_LOG/spark-master.out</span><br><span class="line"></span><br><span class="line">spark-class org.apache.spark.deploy.master.Master \</span><br><span class="line">	--ip $SPARK_MASTER_HOST \</span><br><span class="line">	--port $SPARK_MASTER_PORT \</span><br><span class="line">	--webui-port $SPARK_MASTER_WEBUI_PORT &gt;&gt; $SPARK_MASTER_LOG/spark-master.out</span><br></pre></td></tr></table></figure>
<h3 id="Start-worker"><a href="#Start-worker" class="headerlink" title="Start-worker"></a>Start-worker</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">source /root/.bashrc</span><br><span class="line"></span><br><span class="line">mkdir -p $SPARK_WORKER_LOG</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/spark</span><br><span class="line"></span><br><span class="line">ln -sf /dev/stdout $SPARK_WORKER_LOG/spark-worker.out</span><br><span class="line"></span><br><span class="line">spark-class org.apache.spark.deploy.worker.Worker \</span><br><span class="line">	--webui-port $SPARK_WORKER_WEBUI_PORT \</span><br><span class="line"><span class="meta">	$</span><span class="bash">SPARK_MASTER &gt;&gt; <span class="variable">$SPARK_WORKER_LOG</span>/spark-worker.out</span></span><br></pre></td></tr></table></figure>
<h3 id="Start-shell"><a href="#Start-shell" class="headerlink" title="Start-shell"></a>Start-shell</h3><p>这个<code>start-shell.sh</code>脚本的作用是，在运行容器时，默认就进入<code>spark-shell</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">source /root/.bashrc</span><br><span class="line"></span><br><span class="line">export SPARK_MASTER_HOST=`hostname`</span><br><span class="line"></span><br><span class="line">mkdir -p $SPARK_MASTER_LOG</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/spark</span><br><span class="line"></span><br><span class="line">ln -sf /dev/stdout $SPARK_MASTER_LOG/spark-master.out</span><br><span class="line"></span><br><span class="line">spark-class org.apache.spark.deploy.master.Master \ </span><br><span class="line">	--ip $SPARK_MASTER_HOST \</span><br><span class="line">    --port 	$SPARK_MASTER_PORT \ </span><br><span class="line">    --webui-port $SPARK_MASTER_WEBUI_PORT &gt;&gt; $SPARK_MASTER_LOG/spark-master.out</span><br></pre></td></tr></table></figure>
<blockquote>
<p>脚本创建完成之后赋予可执行权限：<code>chmod +x start-master.sh start-worker.sh start-shell.sh</code></p>
</blockquote>
<h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><p>有了这些脚本之后便可以构建自己所需要的<code>Spark</code>镜像了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/docker/spark</span><br><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure>
<p>内容如下：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> openjdk:<span class="number">8</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> SPARK_MASTER_PORT <span class="number">7077</span></span><br><span class="line"><span class="keyword">ENV</span> SPARK_MASTER_WEBUI_PORT <span class="number">8080</span></span><br><span class="line"><span class="keyword">ENV</span> SPARK_MASTER_LOG /spark/logs</span><br><span class="line"><span class="keyword">ENV</span> SPARK_WORKER_LOG /spark/logs</span><br><span class="line"><span class="keyword">ENV</span> SPARK_VERSION <span class="number">2.4</span>.<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 工具类</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk --update --no-cache add \</span></span><br><span class="line"><span class="bash">        wget tar bash</span></span><br><span class="line"><span class="comment"># Spark 压缩包下载</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> wget https://archive.apache.org/dist/spark/spark-<span class="variable">$&#123;SPARK_VERSION&#125;</span>/spark-<span class="variable">$&#123;SPARK_VERSION&#125;</span>-bin-hadoop2.7.tgz</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压并删除多余压缩包</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> tar -xzf spark-<span class="variable">$&#123;SPARK_VERSION&#125;</span>-bin-hadoop2.7.tgz &amp;&amp; \</span></span><br><span class="line"><span class="bash">    mv spark-<span class="variable">$&#123;SPARK_VERSION&#125;</span>-bin-hadoop2.7 /spark &amp;&amp; \</span></span><br><span class="line"><span class="bash">    rm spark-<span class="variable">$&#123;SPARK_VERSION&#125;</span>-bin-hadoop2.7.tgz</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制环境变量</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> bashrc /root/.bashrc</span></span><br><span class="line"><span class="comment"># 复制启动脚本(包括启动Master和Worker)到容器根目录</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> scripts/* /</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8080</span> <span class="number">7077</span> <span class="number">6066</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认启动 Spark-shell 暂不开启</span></span><br><span class="line"><span class="comment"># ENTRYPOINT ["/start-shell.sh"]</span></span><br></pre></td></tr></table></figure>
<p>然后编译镜像</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t vinci/spark:latest .</span><br></pre></td></tr></table></figure>
<p>编译完成之后进入容器检查一下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --network spark_network \</span><br><span class="line">    vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<p>![启动脚本添加到容器里面 <img src="/2019/04/Docker-Spark-历险记（一）/启动脚本添加到容器里面.png"></p>
<h2 id="编写docker-compose-yml"><a href="#编写docker-compose-yml" class="headerlink" title="编写docker-compose.yml"></a>编写<code>docker-compose.yml</code></h2><p>创建一个新文件：<code>docker-compose.yml</code>，输入以下配置：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">"3.3"</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  spark-master:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">vinci/spark:latest</span></span><br><span class="line"><span class="attr">    container_name:</span> <span class="string">spark-master</span></span><br><span class="line"><span class="attr">    hostname:</span> <span class="string">spark-master</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8080:8080"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"7077:7077"</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">spark-network</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"SPARK_LOCAL_IP=spark-master"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"SPARK_MASTER_PORT=7077"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"SPARK_MASTER_WEBUI_PORT=8080"</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">"/start-master.sh"</span></span><br><span class="line"><span class="attr">  spark-worker:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">vinci/spark:latest</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">spark-master</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">    networks:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">spark-network</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"SPARK_MASTER=spark://spark-master:7077"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"SPARK_WORKER_WEBUI_PORT=8080"</span></span><br><span class="line"><span class="attr">    entrypoint:</span> <span class="string">"/start-worker.sh"</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"./:/local"</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line"><span class="attr">  spark-network:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">bridge</span></span><br><span class="line"><span class="attr">    ipam:</span></span><br><span class="line"><span class="attr">      driver:</span> <span class="string">default</span></span><br></pre></td></tr></table></figure>
<p>接下来要做的事情就很简单了，直接运行以下命令就行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up --scale spark-worker=3</span><br></pre></td></tr></table></figure>
<p>其中<code>--scale</code>作用是：Sets the number of containers to run for a service.</p>
 
<p>运行成功之后可以新建一个<code>SSH</code>连接到虚拟机<code>CentOS</code>上，输入<code>docker container ls</code>查看当前正在运行的容器：</p>
<p>![正在运行的容器 <img src="/2019/04/Docker-Spark-历险记（一）/正在运行的容器.png"></p>
<h2 id="测试一下-1"><a href="#测试一下-1" class="headerlink" title="测试一下"></a>测试一下</h2><p>需要注意的是，这里通过<code>docker-compose</code>启动<code>spark</code>集群的方式，<code>net-work</code>的名字叫做：<strong>spark_spark-network</strong></p>
<img src="/2019/04/Docker-Spark-历险记（一）/docker-network.png">
<p>启动测试容器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it --network spark_spark-network vinci/spark:latest /bin/sh</span><br></pre></td></tr></table></figure>
<p>运行官方示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/spark/bin/spark-submit --master spark://spark-master:7077 --class \</span><br><span class="line">    org.apache.spark.examples.SparkPi \</span><br><span class="line">    /spark/examples/jars/spark-examples_2.11-2.4.0.jar 1000</span><br></pre></td></tr></table></figure>
<p>输出：<code>Pi is roughly 3.1414315514143154</code></p>
<p>至此，本章教程结束。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/04/Docker-Spark-历险记（一）/" data-id="cju9yu8ki00006tu9bhl73e9y" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/04/Docker-Spark-历险记（一）/">Docker Spark 历险记（一）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 姜文奇<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>