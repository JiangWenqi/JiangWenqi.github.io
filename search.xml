<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何在Kubernetes上启用大数据]]></title>
    <url>%2F2019%2F03%2F%E5%A6%82%E4%BD%95%E5%9C%A8Kubernetes%E4%B8%8A%E5%90%AF%E7%94%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[原文地址：https://www.xenonstack.com/insights/big-data-on-kubernetes/ 什么是Apache Hadoop在Kubernetes上启用大数据是平滑数据转换的一种很好的做法。Apache Hadoop是一个框架，允许以分布式模式存储大数据，并在大型数据集上进行分布式处理。它的设计使其可以从单个服务器扩展到数千个服务器。 Hadoop本身旨在检测应用程序层的故障并处理该故障。 Hadoop 3.0是继Hadoop 2之后的主要版本，具有HDFS擦除编码等新功能，提高了性能和可扩展性，多个NameNode等等。 大数据如何在Kubernetes上运作？ 在服务中封装Namenode；Kubernetes pod使用服务资源； Kubernetes服务基本上在群集中提供IP/hostname，用来负载平衡所选容器中的传入请求； pod为NameNode pod提供了一个标签，称为App-namenode并创建服务，即带有该标签的指定pod； 通过有状态集识别数据节点：诸如Kubernetes之类的有状态应用程序提供另一种称为有状态集的资源来帮助这些应用程序； 在有状态集中，每个pod都由其名称，存储和主机名标识； 在单个节点上运行完全分布式HDFS ：在Kubernetes世界中，分布式是以容器为级别的。如果多个节点管理专用磁盘，则在单个节点上运行它的分布式。现在，完全分布式HDFS在一台机器上运行。 大数据在Kubernetes运行的好处 支持多个备用NameNode； 支持多个名称空间的多个NameNode； 存储开销从200％降低到50％； 支持GPU； 节点内磁盘平衡； 支持机会容器和分布式调度； 支持Microsoft Azure Data Lake和Aliyun对象存储系统文件系统连接器。 为什么在Kubernetes运行大数据很重要？ Hadoop 3.0的最低运行时版本是JDK 8。 支持HDFS中的Ensure Coding。 Hadoop Shell脚本重写。 MapReduce任务级别原生优化。 在Hadoop 3.0中引入更强大的YARN。 敏捷性和上市时间。 总拥有成本。 可扩展性和可用性。 如何在Kubernetes上采用大数据？将应用程序部署到Kubernetes 创建一个Dockerfile。 设置群集。 连接到群集。 添加群集和登录Docker注册表。 部署Docker镜像。 构建和部署映像。 拉secret。 镜像名称及注册。 要使用的端口。 将私有映像部署到Kubernetes。 自动化流程部署到Kubernetes。 一些基本的Kubernetes术语 Cluster Node Namespace Deployment Pod Container Service Kubernetes大数据的实践建议 镜像尽可能的小：在开始四处寻找基本镜像之前。一个应用程序需要不超过15MB的大小，使用600MB的镜像是浪费资源。当使用较小的镜像时，使用较小的空间可以更快地构建容器。 使用单个镜像 ：Pod很只运行一个Container是很轻松的。这样可以使得Pod性能更好。当Pods中运行多个Container时，连接，管理和保护微服务是一团糟，因为这些会中断所有通信。 仔细检查基本镜像：许多人在选择镜像时会出错。一切都取决于基本镜像。 Docker Hub上有很多镜像，根据Project的要求选择镜像。在使用基本映像构建Docker映像之前，请仔细检查基本映像。 使用命名空间和标签：在部署映像期间正确定义命名空间和标签。在Kube-cluster内部，有一个名为Namespace的虚拟集群彼此隔离。要选择对象的子集，请使用标签。 在容器内使用非root用户：由于安全原因，始终更倾向在容器内使用非Root用户。非Root用户可以事先配置好该容器的权限。 服务和Pod：服务负责使Pods在网络内可被发现或将其暴露在互联网上。 Pod托管多个容器和存储卷。 熟悉Kube组件 ：用于增强安装程序的性能，安全性和可靠性的多组件。 在服务中封装Namenode。 通过有状态集识别数据节点。 在单个节点上运行完全分布式HDFS。 用于在Kubernetes上启用大数据的工具 Docker kubectl]]></content>
      <categories>
        <category>Kubernets</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Docker</tag>
        <tag>Kubernets</tag>
        <tag>Namespace</tag>
        <tag>Container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Spark 历险记（二）]]></title>
    <url>%2F2019%2F03%2FDocker-Spark-%E5%8E%86%E9%99%A9%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[运行你的第一个Spark应用前言通过这篇文章你能学习到什么，用简单的一句话描述就是： 打包你的第一个Scala程序，并丢到之前创建好的Spark集群上运行。 往复杂了说就是： Docker 部分 继续学习Docker常用操作，如：映射端口，挂载目录，传送变量等； 继续深入学习Dockerfile，熟悉ARG,ENV,RUN,WORKDIR,CMD等指令； Scala 部分 Scala基础语法，Scala编写第一个Spark应用程序； SBT通过配置清单，打包应用程序。 Spark 部分 提交编写好的Scala应用程序，--class主类。 配置Scala运行环境安装Spark是用Scala编写的，所以这里我采用Scala语言进行编写程序。 基于上一篇所提到的openjdk镜像，继续编写Dockerfile： 1234567891011121314151617181920212223242526272829303132333435## Scala and sbt Dockerfile## https://github.com/spikerlabs/scala-sbt (based on https://github.com/hseeberger/scala-sbt)## Pull base imageFROM openjdk:8-alpineARG SCALA_VERSIONARG SBT_VERSIONENV SCALA_VERSION $&#123;SCALA_VERSION:-2.12.8&#125;ENV SBT_VERSION $&#123;SBT_VERSION:-1.2.7&#125;RUN \ echo "$SCALA_VERSION $SBT_VERSION" &amp;&amp; \ mkdir -p /usr/lib/jvm/java-1.8-openjdk/jre &amp;&amp; \ touch /usr/lib/jvm/java-1.8-openjdk/jre/release &amp;&amp; \ apk add --no-cache bash &amp;&amp; \ apk add --no-cache curl &amp;&amp; \ curl -fsL http://downloads.typesafe.com/scala/$SCALA_VERSION/scala-$SCALA_VERSION.tgz | tar xfz - -C /usr/local &amp;&amp; \ ln -s /usr/local/scala-$SCALA_VERSION/bin/* /usr/local/bin/ &amp;&amp; \ scala -version &amp;&amp; \ scalac -versionRUN \ curl -fsL https://github.com/sbt/sbt/releases/download/v$SBT_VERSION/sbt-$SBT_VERSION.tgz | tar xfz - -C /usr/local &amp;&amp; \ $(mv /usr/local/sbt-launcher-packaging-$SBT_VERSION /usr/local/sbt || true) \ ln -s /usr/local/sbt/bin/* /usr/local/bin/ &amp;&amp; \ sbt sbt-version || sbt sbtVersion || trueWORKDIR /projectCMD "/usr/local/bin/sbt" 注意Dockerfile开头的两个参数：SCALA_VERSION和SBT_VERSION是可以用户指定的。 接着编译该Dockerfile： 12345# 注意最后的"."——当前目录docker build -t vinci/scala-sbt:latest \ --build-arg SCALA_VERSION=2.12.8 \ --build-arg SBT_VERSION=1.2.7 \ . 需要一段时间请耐心等待 测试建立一个新的临时交互式容器进行测试： 1docker run -it --rm vinci/scala-sbt:latest /bin/bash 依次输入：scala -version和sbt sbtVersion 当容器里面的界面返回如下信息则说明安装成功。 123456bash-4.4# scala -versionScala code runner version 2.12.8 -- Copyright 2002-2018, LAMP/EPFL and Lightbend, Inc.bash-4.4# sbt sbtVersion[warn] No sbt.version set in project/build.properties, base directory: /local[info] Set current project to local (in build file:/local/)[info] 1.2.7 挂载本地文件为了让我们能够访问我们的本地文件，我们需要将一个卷从我们的工作目录安装到正在运行的容器上的某个位置。 我们只需在run指令里加上-v选项，如下所示： 123mkdir -p /root/docker/projects/MyFirstScalaSparkcd /root/docker/projects/MyFirstScalaSparkdocker run -it --rm -v `pwd`:/project vinci/scala-sbt:latest 注： pwd是指当前目录（Linux 虚拟机：/root/docker/projects/MyFirstScalaSpark）； /project是映射到指容器里面的目录； 没有使用/bin/bash，可以直接登录到SBT控制台。 仔细看之前的Dockerfile配置，最后一行指定了默认执行的命令，倒数第二行指定了工作目录 登陆成功之后会返回如下信息： 12345[root@localhost project]# docker run -it --rm -v `pwd`:/project vinci/scala-sbt:latest[warn] No sbt.version set in project/build.properties, base directory: /local[info] Set current project to local (in build file:/local/)[info] sbt server started at local:///root/.sbt/1.0/server/05a53a1ec23bec1479e9/socksbt:local&gt; 第一个程序配置环境下面便可以开始编写你的第一个Spark程序了。 但是从上节的输出之中还可以看到[warn]，原因是没有设置sbt版本，也就是配置文件的问题。 那么我们在刚才创建的project目录下面新建——build.sbt，内容参考官方文档 1234name := "MyFirstScalaSpark"version := "0.1.0"scalaVersion := "2.11.12"libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.0" 这为我们提供了一个最小的项目定义。 注意：我们已经将Scala版本指定为2.11.12，因为Spark是针对Scala 2.11编译的，但容器上的Scala版本是2.12。 在SBT控制台中，运行reload命令以使用新的构建设置刷新SBT项目： 写代码新建一个SSH连接到CentOS： 创建目录： 123mkdir -p /root/docker/projects/MyFirstScalaSpark/src/main/scala/com/examplecd /root/docker/projects/MyFirstScalaSpark/src/main/scala/com/examplevim MyFirstScalaSpark.scala 内容如下： 12345678910111213141516package com.exampleimport org.apache.spark.sql.SparkSessionobject MyFirstScalaSpark &#123; def main(args: Array[String]) &#123; val SPARK_HOME = sys.env("SPARK_HOME") val logFile = s"$&#123;SPARK_HOME&#125;/README.md" val spark = SparkSession.builder .appName("MyFirstScalaSpark") .getOrCreate() val logData = spark.read.textFile(logFile).cache() val numAs = logData.filter(line =&gt; line.contains("a")).count() val numBs = logData.filter(line =&gt; line.contains("b")).count() println(s"Lines with a: $numAs, Lines with b: $numBs") spark.stop() &#125;&#125; 打包进入到sbt容器，输入 1package 等待很长一段时间，便会出现如下界面，说明打包成功： 提交任务打包好的 jar包在：/root/docker/projects/MyFirstScalaSpark/target/scala-2.11目录下 启动Spark集群（详见第一章）：12cd /root/docker/sparkdocker-compose up --scale spark-worker=2 启动Spark客户端容器1234cd /root/docker/projects/MyFirstScalaSparkdocker run --rm -it -e SPARK_MASTER="spark://spark-master:7077" \ -v `pwd`:/project --network spark_spark-network \ vinci/spark:latest /bin/bash 提交任务进入到Spark客户端容器，输入以下语句： 123spark-submit --master $SPARK_MASTER \ --class com.example.MyFirstScalaSpark \ /project/target/scala-2.11/myfirstscalaspark_2.11-0.1.0.jar 结果输出： Lines with a: 62, Lines with b: 31 执行成功。 本章到此结束。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Spark 历险记（一）]]></title>
    <url>%2F2019%2F03%2FDocker-Spark-%E5%8E%86%E9%99%A9%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一键在Docker上部署属于你自己的Spark计算平台 前言阅读这篇文章之后，你可以学到什么：简单来说就是，可以通过一个命令启动一个 Spark 集群，然后执行你的计算任务。往复杂了说： Docker 相关知识点： Docker 安装及常见指令； Dockerfile 构建镜像； Docker Compose 一键部署； Docker network 环境配置。 Spark 相关知识点： Spark 集群安装及配置； Spark master 及 worker 启动与协作； Spark Job 提交及测试 等等。 准备虚拟机 CentOS-7-x86_64-Minimal-1810.iso 桥接模式 进入虚拟机之后，查询 ip 地址，需要用到：ipconfig 指令，所以输入如下指令： 1yum install net-tools -y 然后便可以使用ifconfig指令查询ip地址：ssh root@192.168.199.240 安装 Docker参照：官方文档 卸载旧版本（可选）12345678sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine 安装 Docker CE 官方介绍有三种方式进行安装，但是我们这里选用最简单的方式，也是官方最为推荐的方式进行安装。 配置 repository123456789# 所需依赖包sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 # 官方推荐稳定版本sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo 安装 Docker CE1sudo yum install docker-ce docker-ce-cli containerd.io 启动 Docker CE1sudo systemctl start docker 检测 Docker CE 安装是否成功1sudo docker run hello-world Docker 切换到国内镜像（可选）国内镜像有很多，如：阿里，中科院大学 等等，这里我选用的docker-cn 具体操作如下： 1vim /etc/docker/daemon.json 加入： 123&#123; "registry-mirrors": ["https://registry.docker-cn.com"]&#125; 然后重启Docker就好了 1sudo systemctl restart docker 搭建Spark服务参见国外文章：https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2 这一节的大体步骤是： 拉去一个基础镜像； 在基础镜像的基础上，添加必要的工具类和Spark安装包； 然后配置脚本，让Spark运行起来。 注：其中最主要有两个文件： 一个是Dockerfile——配置镜像的所有操作； 一个是docker-compose.yml——一键启动集群。 获取Open JDK(基础镜像)这里是通过 Dockerfile来自己创建镜像。 123cd /rootmkdir dockervim Dockerfile 创建一个空白的Dockerfile之后，填入以下配置： 1FROM openjdk:8-alpine 然后便可以进行编译了，最好是打上自己的标签（将 $MYNAME替换成你的名字），如下所示： 123docker build -t $MYNAME/spark:latest .docker build -t vinci/spark:latest . 添加工具类上面所建的 openjdk镜像里面是没有任何工具类的，但是我们下载Spark时需要用到wget，以及tar解压等工具，所以继续在Dockerfile里面添加配置：（新增一行，注意添加 –update） 1RUN apk --update add wget tar bash 然后便可以重新编译镜像了(语句跟之前一样)： 1docker build -t vinci/spark:latest . 下载Spark我们用最新的Spark 2.4.0基于Scala 2.11 和Hadoop 2.7，继续在Dockerfile里新增命令： 1234567# 原作者的链接 404了，我去apache官网上找了个一模一样的RUN wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz# 解压并删除多余压缩包RUN tar -xzf spark-2.4.0-bin-hadoop2.7.tgz &amp;&amp; \ mv spark-2.4.0-bin-hadoop2.7 /spark &amp;&amp; \ rm spark-2.4.0-bin-hadoop2.7.tgz 再次重新编译：docker build -t vinci/spark:latest . 下载耗时较长，请耐心等待 测试一下Spark下载完成之后，便可以run一个容器进行测试： 这里需要注意的是：Spark Master 和 Worker 需要进行通信，所以需要指明端口映射：-p 7077:7077 -p 8080:8080，其中8080端口是WEB-UI的端口： 12345docker run --rm -it --name spark-master --hostname spark-master \ -p 7077:7077 -p 8080:8080 $MYNAME/spark:latest /bin/sh# 这是一个运行完之后就会删除的容器docker run --rm -it --name spark-master --hostname spark-master \ -p 7077:7077 -p 8080:8080 vinci/spark:latest /bin/sh 这样就进入到了容器里面，然后我们新建一个窗口，用SSH连接到虚拟里面，输入docker container ls，可以查看到当前正在运行的容器的状态，如下图所示： 在Spark-master容器中（就是上面进入的容器），输入以下指令启动Spark： 1/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080 然后可以去浏览器确认Spark是否成功启动： 搭建Spark集群以上测试成功之后，退出容器，容器便自动删除了（因为启动容器的时候加了rm选项）。 修改配置文件找到/etc/sysctl.conf 新增一条：net.ipv4.ip_forward=1 重启网络：systemctl restart network 验证配置：sysctl net.ipv4.ip_forward 为本地群集创建一个网络 创建网络非常简单，可以通过运行以下命令来完成： 1docker network create spark_network 启动Spark-Master删除之前建立的Spark-Master容器（默认已经删除了），然后启动指定网络的Spark-Master，只需要加上--network选项，如下所示： 1234567docker run --rm -it --name spark-master --hostname spark-master \ -p 7077:7077 -p 8080:8080 --network spark_network \ $MYNAME/spark:latest /bin/sh docker run --rm -it --name spark-master --hostname spark-master \ -p 7077:7077 -p 8080:8080 --network spark_network \ vinci/spark:latest /bin/sh 进入到容器内部，输入以下指令启动： 1/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080 启动Spark-Worker重新建立一个SSH对话，连接到虚拟机，输入以下指令启动Spark-Worker： 1234567docker run --rm -it --name spark-worker1 --hostname spark-worker1 \ --network spark_network \ $MYNAME/spark:latest /bin/sh docker run --rm -it --name spark-worker1 --hostname spark-worker1 \ --network spark_network \ vinci/spark:latest /bin/sh 进入到worker容器中之后，启动Spark-Worker： 12/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \ --webui-port 8080 spark://spark-master:7077 注：此时回看Spark-Master容器，会发现多了一行日志： 1INFO Master:54 - Registering worker 172.18.0.3:36486 with 2 cores, 1024.0 MB RAM 至此，Spark 集群已经安装成功了 Spark集群实践一般是一主两从集群架构，所以我们还可以新建一个Spark-Work2容器，指令跟之前相似： 123docker run --rm -it --name spark-worker2 --hostname spark-worker2 \ --network spark_network \ vinci/spark:latest /bin/sh 进入spark-worker2容器之后，继续启动Spark-Worker服务： 12/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \ --webui-port 8080 spark://spark-master:7077 然后宿主机，浏览器输入：虚拟机IP:8080，验证Spark服务： 运行计算再次启动一个容器进入到spark-network中： 12345docker run --rm -it --network spark_network \ $MYNAME/spark:latest /bin/sh docker run --rm -it --network spark_network \ vinci/spark:latest /bin/sh 运行官方提供的样例： 123/spark/bin/spark-submit --master spark://spark-master:7077 --class \ org.apache.spark.examples.SparkPi \ /spark/examples/jars/spark-examples_2.11-2.4.0.jar 1000 运行之后会看到哗啦啦的日志输出，我们也可以通过Web-UI来进行监控。 Docker Compose通过Docker Compose可以极大简化我们的安装部署流程。 这一节将对之前的知识点进行汇总，所以嫌麻烦的可以不看前面，直接看这里。 配置 Docker Compose12345sudo curl -L "https://github.com/docker/compose/releases/download/1.23.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composedocker-compose --version 环境变量为容器添加Spark的环境变量，这样就不需要输入前面一大串绝对路径了。 12cd /root/docker/sparkvim bashrc 添加环境变量 12SPARK_HOME=/sparkPATH=$PATH:$SPARK_HOME/bin 启动脚本启动脚本也就是之前我们进入容器输入的启动spark-master或者spark-worker的命令。 注意脚本的第一行必须是：#!/bin/bash 12345mkdir -p /root/docker/spark/scriptscd /root/docker/spark/scriptsvim start-master.shvim start-worker.shvim start-all.sh Start-master12345678910111213141516#!/bin/bashsource /root/.bashrcexport SPARK_MASTER_HOST=`hostname`mkdir -p $SPARK_MASTER_LOGexport SPARK_HOME=/sparkln -sf /dev/stdout $SPARK_MASTER_LOG/spark-master.outspark-class org.apache.spark.deploy.master.Master \ --ip $SPARK_MASTER_HOST \ --port $SPARK_MASTER_PORT \ --webui-port $SPARK_MASTER_WEBUI_PORT &gt;&gt; $SPARK_MASTER_LOG/spark-master.out Start-worker12345678910111213#!/bin/bashsource /root/.bashrcmkdir -p $SPARK_WORKER_LOGexport SPARK_HOME=/sparkln -sf /dev/stdout $SPARK_WORKER_LOG/spark-worker.outspark-class org.apache.spark.deploy.worker.Worker \ --webui-port $SPARK_WORKER_WEBUI_PORT \ $SPARK_MASTER &gt;&gt; $SPARK_WORKER_LOG/spark-worker.out Start-shell这个start-shell.sh脚本的作用是，在运行容器时，默认就进入spark-shell： 12345678910111213141516#!/bin/bashsource /root/.bashrcexport SPARK_MASTER_HOST=`hostname`mkdir -p $SPARK_MASTER_LOGexport SPARK_HOME=/sparkln -sf /dev/stdout $SPARK_MASTER_LOG/spark-master.outspark-class org.apache.spark.deploy.master.Master \ --ip $SPARK_MASTER_HOST \ --port $SPARK_MASTER_PORT \ --webui-port $SPARK_MASTER_WEBUI_PORT &gt;&gt; $SPARK_MASTER_LOG/spark-master.out 脚本创建完成之后赋予可执行权限：chmod +x start-master.sh start-worker.sh start-shell.sh Dockerfile有了这些脚本之后便可以构建自己所需要的Spark镜像了。 12cd /root/docker/sparkvim Dockerfile 内容如下： 1234567891011121314151617181920212223242526272829FROM openjdk:8-alpineENV SPARK_MASTER_PORT 7077ENV SPARK_MASTER_WEBUI_PORT 8080ENV SPARK_MASTER_LOG /spark/logsENV SPARK_WORKER_LOG /spark/logsENV SPARK_VERSION 2.4.0# 工具类RUN apk --update --no-cache add \ wget tar bash# Spark 压缩包下载RUN wget https://archive.apache.org/dist/spark/spark-$&#123;SPARK_VERSION&#125;/spark-$&#123;SPARK_VERSION&#125;-bin-hadoop2.7.tgz# 解压并删除多余压缩包RUN tar -xzf spark-$&#123;SPARK_VERSION&#125;-bin-hadoop2.7.tgz &amp;&amp; \ mv spark-$&#123;SPARK_VERSION&#125;-bin-hadoop2.7 /spark &amp;&amp; \ rm spark-$&#123;SPARK_VERSION&#125;-bin-hadoop2.7.tgz# 复制环境变量COPY bashrc /root/.bashrc# 复制启动脚本(包括启动Master和Worker)到容器根目录COPY scripts/* /# 暴露端口EXPOSE 8080 7077 6066# 默认启动 Spark-shell 暂不开启# ENTRYPOINT ["/start-shell.sh"] 然后编译镜像 1docker build -t vinci/spark:latest . 编译完成之后进入容器检查一下 12docker run --rm -it --network spark_network \ vinci/spark:latest /bin/sh 编写docker-compose.yml创建一个新文件：docker-compose.yml，输入以下配置： 1234567891011121314151617181920212223242526272829303132333435version: "3.3"services: spark-master: image: vinci/spark:latest container_name: spark-master hostname: spark-master ports: - "8080:8080" - "7077:7077" networks: - spark-network environment: - "SPARK_LOCAL_IP=spark-master" - "SPARK_MASTER_PORT=7077" - "SPARK_MASTER_WEBUI_PORT=8080" command: "/start-master.sh" spark-worker: image: vinci/spark:latest depends_on: - spark-master ports: - 8080 networks: - spark-network environment: - "SPARK_MASTER=spark://spark-master:7077" - "SPARK_WORKER_WEBUI_PORT=8080" entrypoint: "/start-worker.sh" volumes: - "./:/local"networks: spark-network: driver: bridge ipam: driver: default 接下来要做的事情就很简单了，直接运行以下命令就行： 1docker-compose up --scale spark-worker=3 其中--scale作用是：Sets the number of containers to run for a service. 运行成功之后可以新建一个SSH连接到虚拟机CentOS上，输入docker container ls查看当前正在运行的容器： 测试一下需要注意的是，这里通过docker-compose启动spark集群的方式，net-work的名字叫做：spark_spark-network 启动测试容器： 1docker run --rm -it --network spark_spark-network vinci/spark:latest /bin/sh 运行官方示例： 123/spark/bin/spark-submit --master spark://spark-master:7077 --class \ org.apache.spark.examples.SparkPi \ /spark/examples/jars/spark-examples_2.11-2.4.0.jar 1000 输出：Pi is roughly 3.1414315514143154 至此，本章教程结束。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Livy REST API 封装（Java）]]></title>
    <url>%2F2019%2F03%2FLivy-REST-API-%E5%B0%81%E8%A3%85%EF%BC%88Java%EF%BC%89%2F</url>
    <content type="text"><![CDATA[参考文章如下： https://blog.csdn.net/camel84/article/details/81990383 https://cloud.tencent.com/developer/article/1078857 项目地址：https://github.com/JiangWenqi/LivyRESTAPI 前言 Livy is an open source REST interface for interacting with Apache Spark from anywhere. It supports executing snippets of code or programs in a Spark context that runs locally or in Apache Hadoop YARN. Interactive Scala, Python and R shells Batch submissions in Scala, Java, Python Multiple users can share the same server (impersonation support) Can be used for submitting jobs from anywhere with REST Does not require any code change to your programs 以上是Livy的官方介绍，具体使用请参照这篇文章。 大体思路是用 Java 模拟发送请求报文给 Livy， 项目依赖12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.livy/livy-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.livy&lt;/groupId&gt; &lt;artifactId&gt;livy-core_2.11&lt;/artifactId&gt; &lt;version&gt;0.5.0-incubating&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.livy/livy-rsc --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.livy&lt;/groupId&gt; &lt;artifactId&gt;livy-rsc&lt;/artifactId&gt; &lt;version&gt;0.5.0-incubating&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.json-lib&lt;/groupId&gt; &lt;artifactId&gt;json-lib&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;classifier&gt;jdk15&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.ibatis/ibatis-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.ibatis&lt;/groupId&gt; &lt;artifactId&gt;ibatis-core&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Spark Job封装 Name Description Type file File containing the application to execute path (required) proxyUser User to impersonate when running the job string className Application Java/Spark main class string args Command line arguments for the application list of strings jars jars to be used in this session list of strings pyFiles Python files to be used in this session list of strings files files to be used in this session list of strings driverMemory Amount of memory to use for the driver process string driverCores Number of cores to use for the driver process int executorMemory Amount of memory to use per executor process string executorCores Number of cores to use for each executor int numExecutors Number of executors to launch for this session int archives Archives to be used in this session List of string queue The name of the YARN queue to which submitted string name The name of this session string conf Spark configuration properties Map of key=val 这是原本是需要自己根据所需要的参数拼接 json，作为request body上传到Livy服务器，执行相应任务。 但是为了方便起见，我对该request body进行了封装。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220package space.jwqwy.livy.entiy;import java.util.List;import java.util.Map;/** * Livy REST API 封装 * * @author Vinci * Create: 2019/02/18 16:37 * Description: Request Body Livy 批处理任务 属性封装 */public class SparkJob &#123; /** * 必须有 * 包含需要执行应用的文件，主要是jar包 */ private String file; /** * User to impersonate when running the job */ private String proxyUser; /** * Application Java/Spark main class * 主类 */ private String className; /** * Command line arguments for the application * 参数 */ private List&lt;String&gt; args; /** * jars to be used in this session * 这个任务里面用到的其他 jar 包 */ private List&lt;String&gt; jars; /** * Python files to be used in this session */ private List&lt;String&gt; pyFiles; /** * files to be used in this session */ private List&lt;String&gt; files; /** * Amount of memory to use for the driver process */ private String driverMemory; /** * Number of cores to use for the driver process */ private int driverCores; /** * Amount of memory to use per executor process */ private String executorMemory; /** * Number of cores to use for each executor */ private int executorCores; /** * Number of executors to launch for this session */ private int numExecutors; /** * Archives to be used in this session */ private List&lt;String&gt; archives; /** * The name of the YARN queue to which submitted */ private String queue; /** * The name of this session * 任务名称 */ private String name; /** * Spark configuration properties * spark 配置文件 */ private Map&lt;String, Object&gt; conf; public String getFile() &#123; return file; &#125; public void setFile(String file) &#123; this.file = file; &#125; public String getProxyUser() &#123; return proxyUser; &#125; public void setProxyUser(String proxyUser) &#123; this.proxyUser = proxyUser; &#125; public String getClassName() &#123; return className; &#125; public void setClassName(String className) &#123; this.className = className; &#125; public List&lt;String&gt; getArgs() &#123; return args; &#125; public void setArgs(List&lt;String&gt; args) &#123; this.args = args; &#125; public List&lt;String&gt; getJars() &#123; return jars; &#125; public void setJars(List&lt;String&gt; jars) &#123; this.jars = jars; &#125; public List&lt;String&gt; getPyFiles() &#123; return pyFiles; &#125; public void setPyFiles(List&lt;String&gt; pyFiles) &#123; this.pyFiles = pyFiles; &#125; public List&lt;String&gt; getFiles() &#123; return files; &#125; public void setFiles(List&lt;String&gt; files) &#123; this.files = files; &#125; public String getDriverMemory() &#123; return driverMemory; &#125; public void setDriverMemory(String driverMemory) &#123; this.driverMemory = driverMemory; &#125; public int getDriverCores() &#123; return driverCores; &#125; public void setDriverCores(int driverCores) &#123; this.driverCores = driverCores; &#125; public String getExecutorMemory() &#123; return executorMemory; &#125; public void setExecutorMemory(String executorMemory) &#123; this.executorMemory = executorMemory; &#125; public int getExecutorCores() &#123; return executorCores; &#125; public void setExecutorCores(int executorCores) &#123; this.executorCores = executorCores; &#125; public int getNumExecutors() &#123; return numExecutors; &#125; public void setNumExecutors(int numExecutors) &#123; this.numExecutors = numExecutors; &#125; public List&lt;String&gt; getArchives() &#123; return archives; &#125; public void setArchives(List&lt;String&gt; archives) &#123; this.archives = archives; &#125; public String getQueue() &#123; return queue; &#125; public void setQueue(String queue) &#123; this.queue = queue; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Map&lt;String, Object&gt; getConf() &#123; return conf; &#125; public void setConf(Map&lt;String, Object&gt; conf) &#123; this.conf = conf; &#125;&#125; Http UtilsHttpUtils是比较这个项目最为核心的工具类，用来模拟发送：POST,GET,DELETE 等报文请求。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110package space.jwqwy.livy.util;import org.apache.http.HttpEntity;import org.apache.http.HttpResponse;import org.apache.http.client.methods.HttpDelete;import org.apache.http.client.methods.HttpGet;import org.apache.http.client.methods.HttpPost;import org.apache.http.entity.StringEntity;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.util.EntityUtils;import java.io.IOException;import java.util.Map;/** * Livy REST API 封装 * * @author Vinci * Create: 2019/02/19 15:35 * Description: Http 报文 */public class HttpUtils &#123; /** * HttpGET请求 * * @param url 链接 * @param headers 报文头 * @return 结果 */ public static String getAccess(String url, Map&lt;String, String&gt; headers) &#123; String result = null; CloseableHttpClient httpClient = HttpClients.createDefault(); HttpGet httpGet = new HttpGet(url); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; headers.forEach(httpGet::addHeader); &#125; try &#123; HttpResponse response = httpClient.execute(httpGet); HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * HttpDelete请求 * * @param url 链接 * @param headers 报文头 * @return 结果 */ public static String deleteAccess(String url, Map&lt;String, String&gt; headers) &#123; String result = null; CloseableHttpClient httpClient = HttpClients.createDefault(); HttpDelete httpDelete = new HttpDelete(url); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; headers.forEach(httpDelete::addHeader); &#125; try &#123; HttpResponse response = httpClient.execute(httpDelete); HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * HttpPost请求 * * @param url url * @param headers 请求报文头 * @param data 数据 * @return 结果 */ public static String postAccess(String url, Map&lt;String, String&gt; headers, String data) &#123; String result = null; CloseableHttpClient httpClient = HttpClients.createDefault(); HttpPost post = new HttpPost(url); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; headers.forEach(post::addHeader); &#125; try &#123; StringEntity entity = new StringEntity(data); entity.setContentEncoding("UTF-8"); entity.setContentType("application/json"); post.setEntity(entity); HttpResponse response = httpClient.execute(post); HttpEntity resultEntity = response.getEntity(); result = EntityUtils.toString(resultEntity); return result; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125;&#125; Livy Service及实现类Livy Service封装了一共有八个方法，具体作用看代码注释 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package space.jwqwy.livy.service;import space.jwqwy.livy.entiy.SparkJob;import space.jwqwy.livy.eum.SparkJobState;import java.util.Map;/** * Livy REST API 封装 * * @author Vinci * Create: 2019/02/19 15:01 * Description: 如何通过Livy的RESTful API接口向CDH集群提交作业 */public interface LivyService &#123; /** * 运行一个 SparkJob 一直等到他运行完成之后才会有返回值 * * @param job SparkJob * @return 任务是否正确运行结束 */ boolean runSparkJob(SparkJob job); /** * 后台启动一个 SparkJob * * @param job SparkJob * @return SparkJob 的 batch session ID */ int runSparkJobBackground(SparkJob job); /** * 启动一个 session 运行 SparkJob, 不需要等待是否运行成功 * * @param job SparkJob * @return SparkJob 的 batch session ID */ int startSparkJob(SparkJob job); /** * 查询所有的 活跃的 Spark Job * * @return 所有活跃的 Spark Job = batch session */ Map&lt;String, Object&gt; getActiveSparkJobs(); /** * 查询具体的且活跃的 Spark Job 信息 * * @param sparkJobID SparkJob 的 ID（batch session ID） * @return Spark Job 信息 ，具体的 batch session 信息 */ Map&lt;String, Object&gt; getSparkJobInfo(int sparkJobID); /** * 查询具体的且活跃的 Spark Job 状态 * * @param sparkJobID SparkJob 的 ID（batch session ID） * @return Spark Job 状态 ，具体的 batch session 状态 */ SparkJobState getSparkJobState(int sparkJobID); /** * 查询具体的且活跃的 Spark Job 日志 * * @param sparkJobID SparkJob 的 ID（batch session ID） * @return Spark Job 日志 ，具体的 batch session 日志 */ Map&lt;String, Object&gt; getSparkJoblog(int sparkJobID); /** * Kills the Batch job. * * @param sparkJobID SparkJob 的 ID（batch session ID） * @return msg * &#123; * "msg": "deleted" * &#125; */ Map&lt;String, Object&gt; deleteSparkJob(int sparkJobID);&#125; Livy Service实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226package space.jwqwy.livy.service.impl;import net.sf.json.JSONObject;import net.sf.json.JsonConfig;import net.sf.json.util.PropertyFilter;import org.apache.log4j.Logger;import space.jwqwy.livy.common.Constants;import space.jwqwy.livy.entiy.SparkJob;import space.jwqwy.livy.eum.SparkJobState;import space.jwqwy.livy.service.LivyService;import space.jwqwy.livy.util.HttpUtils;import space.jwqwy.livy.util.PropertiesUtil;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.Properties;/** * Livy REST API 封装 * * @author Vinci * Create: 2019/02/19 15:12 * Description: Livy Service实现类 */public class LivyServiceImpl implements LivyService &#123; private static Logger logger = Logger.getLogger(LivyServiceImpl.class); private static String LIVY_URL = ""; public LivyServiceImpl() &#123; try &#123; Properties properties = PropertiesUtil.getProperties("properties/livy.properties"); LIVY_URL = String.valueOf(properties.get("LIVY_URL")); &#125; catch (IOException e) &#123; logger.error("请检查配置文件，找不到 Livy URL"); e.printStackTrace(); &#125; &#125; @Override public boolean runSparkJob(SparkJob sparkJob) &#123; int sparkJobID = startSparkJob(sparkJob); while (true) &#123; SparkJobState sparkJobState = getSparkJobState(sparkJobID); switch (sparkJobState) &#123; case SHUTTING_DOWN: return false; case ERROR: return false; case DEAD: return false; case SUCCESS: return true; default: &#125; try &#123; // 休眠3s Thread.sleep(3000); &#125; catch (Exception ex) &#123; logger.error(ex.getMessage()); &#125; &#125; &#125; @Override public int runSparkJobBackground(SparkJob sparkJob) &#123; return startSparkJob(sparkJob); &#125; @Override public int startSparkJob(SparkJob sparkJob) &#123; int sparkJobID = -1; JSONObject batchSession = createBatch(parse2Json(sparkJob)); String state = batchSession.getString(Constants.LIVY_SESSION_STATE); // 如果 session 状态为 不为 dead 或者 error ，则返回 session id SparkJobState sparkJobState = SparkJobState.fromDescription(state); if (sparkJobState != SparkJobState.DEAD &amp;&amp; sparkJobState != SparkJobState.ERROR) &#123; sparkJobID = (int) batchSession.get(Constants.LIVY_SESSION_ID); &#125; else &#123; logger.error("================ 创建Spark 任务失败=======================\n"); logger.error("=====================失败原因:==========================\n" + batchSession.toString()); &#125; return sparkJobID; &#125; @Override public Map&lt;String, Object&gt; getActiveSparkJobs() &#123; return getBatchSessions(); &#125; @Override public Map&lt;String, Object&gt; getSparkJobInfo(int sparkJobID) &#123; return getBatchSession(sparkJobID); &#125; @Override public SparkJobState getSparkJobState(int sparkJobID) &#123; return getBatchSessionState(sparkJobID); &#125; @Override public Map&lt;String, Object&gt; getSparkJoblog(int sparkJobID) &#123; return getBatchSessionLog(sparkJobID); &#125; @Override public Map&lt;String, Object&gt; deleteSparkJob(int sparkJobID) &#123; return deleteBatchSession(sparkJobID); &#125; /** * 过滤器，把默认值的参数剔除掉 * * @param job Livy任务 * @return jobJson */ private JSONObject parse2Json(SparkJob job) &#123; // 过滤器，把默认值的参数剔除掉 PropertyFilter filter = (source, name, value) -&gt; &#123; // 如果为数字则判断是否为0（默认值），如果为0，则为 true if (value instanceof Number &amp;&amp; (int) value == 0) &#123; return true; &#125; else &#123; return null == value; &#125; &#125;; JsonConfig jsonConfig = new JsonConfig(); jsonConfig.setJsonPropertyFilter(filter); return JSONObject.fromObject(job, jsonConfig); &#125; /** * 创建一个 Batch Session 执行 SparkJob * * @param sparkJobJson sparkJob json 形式 * @return 该 job 的 batch session 信息 */ private JSONObject createBatch(JSONObject sparkJobJson) &#123; // 将 Map 转为字符串 String sparkJobJsonStr = JSONObject.fromObject(sparkJobJson).toString(); return createBatch(sparkJobJsonStr); &#125; /** * 创建一个 Batch Session 执行 SparkJob * * @param sparkJobJsonStr sparkJob 字符串形式 * @return 该 job 的 batch session 信息 */ private JSONObject createBatch(String sparkJobJsonStr) &#123; JSONObject resultJson = null; Map&lt;String, String&gt; headers = new HashMap&lt;&gt;(4); headers.put("Accept", "application/json"); headers.put("Content-Type", "application/json"); headers.put("Accept-Charset", "utf-8"); String result = HttpUtils.postAccess(LIVY_URL + "/batches", headers, sparkJobJsonStr); if (result != null) &#123; resultJson = JSONObject.fromObject(result); &#125; else &#123; logger.error("\n==============Livy 提交批任务失败==================\n"); &#125; return resultJson; &#125; private Map&lt;String, Object&gt; getBatchSessions() &#123; JSONObject resultJson = null; String result = HttpUtils.getAccess(LIVY_URL + "/batches", null); if (result != null) &#123; resultJson = JSONObject.fromObject(result); &#125; else &#123; logger.error("\n==============Livy 查询批任务失败==================\n"); &#125; return resultJson; &#125; private Map&lt;String, Object&gt; getBatchSession(int batchID) &#123; JSONObject resultJson = null; String result = HttpUtils.getAccess(LIVY_URL + "/batches/" + batchID, null); if (result != null) &#123; resultJson = JSONObject.fromObject(result); &#125; else &#123; logger.error("\n==============Livy 查询具体任务失败，任务编号为：\n" + batchID + "\n"); &#125; return resultJson; &#125; private SparkJobState getBatchSessionState(int batchID) &#123; SparkJobState sparkJobState = null; String result = HttpUtils.getAccess(LIVY_URL + "/batches/" + batchID + "/state", null); if (result != null) &#123; JSONObject resultJson = JSONObject.fromObject(result); String state = resultJson.getString("state"); sparkJobState = SparkJobState.fromDescription(state); &#125; else &#123; logger.error("\n==============Livy 查询具体任务状态失败，任务编号为：\n" + batchID); &#125; return sparkJobState; &#125; private Map&lt;String, Object&gt; getBatchSessionLog(int batchID) &#123; JSONObject resultJson = null; String result = HttpUtils.getAccess(LIVY_URL + "/batches/" + batchID + "/log", null); if (result != null) &#123; resultJson = JSONObject.fromObject(result); &#125; else &#123; logger.error("\n==============Livy 查询具体任务日志失败，任务编号为：\n" + batchID + "\n"); &#125; return resultJson; &#125; private Map&lt;String, Object&gt; deleteBatchSession(int batchID) &#123; JSONObject resultJson = null; String result = HttpUtils.deleteAccess(LIVY_URL + "/batches/" + batchID, null); if (result != null) &#123; resultJson = JSONObject.fromObject(result); &#125; else &#123; logger.error("\n==============Livy 删除具体任务失败，任务编号为：\n" + batchID + "\n"); &#125; return resultJson; &#125;&#125; Livy Service 测试第一步：上传 jar 包上传测试所用的jar包到hdfs。 123export HADOOP_USER_NAME=hdfs$&#123;HADOOP_HOME&#125;/bin/hdfs dfs -mkdir /testJars$&#123;HADOOP_HOME&#125;/bin/hdfs dfs -put /opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/examples/jars/spark-examples_2.11-2.3.0.cloudera4.jar /testJars/ 第二步：创建 Spark Job123456SparkJob job = new SparkJob();job.setFile("hdfs://192.168.1.170:8020/testJars/spark-examples_2.11-2.3.0.cloudera4.jar");job.setClassName("org.apache.spark.examples.SparkPi");job.setName("SparkPi");job.setExecutorCores(3); 第三部：执行任务，查询任务状态等操作12345678910111213141516171819202122int sparkJobID = livyService.startSparkJob(job);if (sparkJobID &gt; 0) &#123; System.out.println("\n创建任务，任务ID为：\n" + sparkJobID); Map&lt;String, Object&gt; activeSparkJobs = livyService.getActiveSparkJobs(); System.out.println("\n查询当前所有任务：\n" + activeSparkJobs.toString()); Map&lt;String, Object&gt; info = livyService.getSparkJobInfo(sparkJobID); System.out.println("\n查询任务ID为" + sparkJobID + "的任务详情:\n" + info.toString()); SparkJobState state = livyService.getSparkJobState(sparkJobID); System.out.println("\n查询任务ID为" + sparkJobID + "的任务状态:\n" + state); Map&lt;String, Object&gt; log = livyService.getSparkJoblog(sparkJobID); System.out.println("\n查询任务ID为" + sparkJobID + "的任务日志:\n" + log.toString()); // Map&lt;String, Object&gt; del = livyService.deleteSparkJob(sparkJobID); // System.out.println("删除任务ID为" + sparkJobID + "\n" + del.toString());&#125;// 执行任务，一直到任务结束// System.out.println(runSparkJob(job));]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>Livy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群配置Livy]]></title>
    <url>%2F2019%2F02%2F%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AELivy%2F</url>
    <content type="text"><![CDATA[集群配置Livy一、下载Livy 官网地址：http://livy.incubator.apache.org/download/ 具体指令： 1234# 进入指定目录cd /home/admin/soft# 下载文件wget http://mirror.bit.edu.cn/apache/incubator/livy/0.5.0-incubating/livy-0.5.0-incubating-bin.zip 二、解压1234567unzip livy-0.5.0-incubating-bin.zip# 重命名mv livy-0.5.0-incubating-bin livy-0.5.0# 复制到 /etc 目录下cp -r livy-0.5.0 /etc/livy 三、更改配置文件1234567cd /etc# 给文件夹赋予权限chmod +777 livy# 配置文件cd /etc/livy/confcp livy-env.sh.template livy-env.shvim livy-env.sh 1234# - HADOOP_CONF_DIR Directory containing the Hadoop / YARN configuration to use.HADOOP_CONF_DIR=/etc/hadoop/conf# - SPARK_HOME Spark which you would like to use in Livy.SPARK_HOME=/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2 四、启动服务12345cd /etc/livy# 新建日志目录，并给权限mkdir logschmod +777 logs./bin/livy-server 启动成功： 12#后台模式./bin/livy-server start]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>Livy</tag>
        <tag>Cloudera Manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建大数据集群(Cloudera Manager)]]></title>
    <url>%2F2018%2F12%2F%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4-Cloudera-Manager%2F</url>
    <content type="text"><![CDATA[官方安装教程：https://www.cloudera.com/documentation/enterprise/5-13-x/topics/installation_installation.html 官网推荐方式官方共给出了3中安装方式： 第一种方法在线安装，不推荐（我自己安装失败了，因为很多网站是被墙，不适合国内大环境）； 第二种方法需要下载很多包； 第三种方法对系统侵入性最小,最大优点可实现全离线安装，而且重装什么的都非常方便。后期的集群统一包升级也非常好。这也是我之所以选择离线安装的原因。 前期准备 内容 版本 CentOS 7 64位 JDK 1.8 Cloudera Manager 6.1.0 本次安装一共使用3台服务器，主要用户测试。 服务名 内网IP 用途 master 192.168.199.234 主，按照CM slave01 192.168.199.146 从 slave02 192.168.199.153 从 可以搭建三台虚拟机，其中master内存在8G以上，slave内存在4G以上，每个虚机的硬盘空间100G+ 具体步骤0. 安装必要开发工具（可选）12345678# 更新包yum update# 必要的网络工具，如：ifconfigyum -y install net-tools# 下载工具yum -y install wget# Vimyum -y install vim 1. 安装Oracle JDK下载64位JDK由于版权原因，Linux发行版并没有包含官方版的Oracle JDK，必须自己从官网上下载安装。 Oracle官网用Cookie限制下载方式，使得眼下只能用浏览器进行下载，使用其他方式可能会导致下载失败。 12# 下载Oracle JDK 1.8wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-linux-x64.tar.gz 说明： wget：下载方式 –no-check-certificate：用于禁止检查证书 –no-cookies：用于禁用Cookies –header=header-line：用于定义请求头信息 http…：即为jdk1.8下载链接 配置 解压JDK到指定文件夹/usr/java/jdk-version，如：/usr/java/jdk1.8.0_201 1234# 创建目录mkdir /usr/java# 解压到指定文件夹tar -zxvf jdk-8u201-linux-x64.tar.gz -C /usr/java/ 配置JAVA_HOME 12345vim /etc/profile# 在文件底部输入export JAVA_HOME=/usr/java/jdk1.8.0_201export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 配置文件生效，并检查安装是否成功 12source /etc/profilejava -version 2. 下载Cloudera Manager必要软件 创建对应目录 12mkdir -p ~/soft/cm6.1.0/RPMScd ~/soft/cm6.1.0/RPMS 登录官方网站下载所有的RPM包 12345wget -b https://archive.cloudera.com/cm6/6.1.0/redhat7/yum/RPMS/x86_64/cloudera-manager-agent-6.1.0-769885.el7.x86_64.rpmwget -b https://archive.cloudera.com/cm6/6.1.0/redhat7/yum/RPMS/x86_64/cloudera-manager-daemons-6.1.0-769885.el7.x86_64.rpmwget -b https://archive.cloudera.com/cm6/6.1.0/redhat7/yum/RPMS/x86_64/cloudera-manager-server-6.1.0-769885.el7.x86_64.rpmwget -b https://archive.cloudera.com/cm6/6.1.0/redhat7/yum/RPMS/x86_64/cloudera-manager-server-db-2-6.1.0-769885.el7.x86_64.rpmwget -b https://archive.cloudera.com/cm6/6.1.0/redhat7/yum/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm 下载rpm仓库文件 12cd ~/soft/cm6.1.0wget https://archive.cloudera.com/cm6/6.1.0/redhat7/yum/cloudera-manager.repo 下载在线安装文件 12cd ~/soft/cm6.1.0wget https://archive.cloudera.com/cm6/6.1.0/cloudera-manager-installer.bin 下载parcel文件 1234mkdir -p ~/soft/cm6.1.0/parcelscd ~/soft/cm6.1.0/parcelswget -b https://archive.cloudera.com/cdh6/6.1.0/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcelwget -b https://archive.cloudera.com/cdh6/6.1.0/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel.sha256 注：下载时间较长，所以采用后台下载。 3. 服务器环境准备修改 hostname 及 hosts 针对所有节点操作 为了更好地区分服务器和更快捷地操作，修改 hostname 及 hosts。 node00（主节点）：hostnamectl --static set-hostname master node01（从节点）：hostnamectl --static set-hostname slave01 node02（从节点）：hostnamectl --static set-hostname slave02 12345vim /etc/hosts# 在最后添加192.168.199.234 master192.168.199.146 slave01192.168.199.153 slave02 注：重启生效 关闭防火墙及selinux 针对所有节点操作 关闭防火墙 123systemctl stop firewalld.service #停止firewallsystemctl disable firewalld.service #禁止firewall开机启动firewall-cmd --state #查看默认防火墙状态（关闭后显示notrunning，开启后显示running） 关闭selinux： 1vim /etc/selinux/config 找到SELINUX改为： 1SELINUX=disabled SSH免密登录 针对所有节点操作 现在 master 上执行 1234#一路回车到完成ssh-keygen -t rsa #将公钥拷贝到本机的authorized_keys上ssh-copy-id -i ~/.ssh/id_rsa.pub root@master 再在其他节点分别执行以下命令： 1234#一路回车到完成ssh-keygen -t rsa #注意此处不变，将公钥拷贝到master的authorized_keys上ssh-copy-id -i ~/.ssh/id_rsa.pub root@master 在master上，将authorized_keys分发到其他节点服务器： 12scp ~/.ssh/authorized_keys root@slave01:~/.ssh/scp ~/.ssh/authorized_keys root@slave02:~/.ssh/ 安装ntp时间同步软件 针对所有节点操作 123yum install ntp -y# 开机启动systemctl enable ntpd 重启服务器 4. 安装Cloudera Manager 和 CDH准备所需文件 针对所有节点操作 分配主节点和从节点所需要的文件： 主节点（就是之前下载的所有文件） 123456789cloudera-manager-agent-6.1.0-769885.el7.x86_64.rpmcloudera-manager-daemons-6.1.0-769885.el7.x86_64.rpmcloudera-manager-server-6.1.0-769885.el7.x86_64.rpmcloudera-manager-server-db-2-6.1.0-769885.el7.x86_64.rpmoracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpmcloudera-manager-installer.bincloudera-manager.repoCDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcelCDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel.sha256 从节点需要的文件有三个 123456789101112131415161718192021# 在从节点上建立相应目录mkdir -p ~/soft/cm6.1.0/RPMSmkdir -p ~/soft/cm6.1.0/parcels# 在主节点上执行scp -r ~/soft/cm6.1.0/RPMS/cloudera-manager-agent-6.1.0-769885.el7.x86_64.rpm root@slave01:~/soft/cm6.1.0/RPMSscp -r ~/soft/cm6.1.0/RPMS/cloudera-manager-daemons-6.1.0-769885.el7.x86_64.rpm root@slave01:~/soft/cm6.1.0/RPMSscp -r ~/soft/cm6.1.0/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel root@slave01:~/soft/cm6.1.0/parcelsscp -r ~/soft/cm6.1.0/cloudera-manager.repo root@slave01:~/soft/cm6.1.0scp -r ~/soft/cm6.1.0/RPMS/cloudera-manager-agent-6.1.0-769885.el7.x86_64.rpm root@slave02:~/soft/cm6.1.0/RPMSscp -r ~/soft/cm6.1.0/RPMS/cloudera-manager-daemons-6.1.0-769885.el7.x86_64.rpm root@slave02:~/soft/cm6.1.0/RPMSscp -r ~/soft/cm6.1.0/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel root@slave02:~/soft/cm6.1.0/parcelsscp -r ~/soft/cm6.1.0/cloudera-manager.repo root@slave02:~/soft/cm6.1.0# 离线 parcelcp -p ~/soft/cm6.1.0/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel /opt/cloudera/parcel-repo/cp -p ~/soft/cm6.1.0/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel.sha256 /opt/cloudera/parcel-repo/chown -R cloudera-scm.cloudera-scm /var/lib/cloudera-scm-serverchown cloudera-scm.cloudera-scm /opt -Rchown cloudera-scm.cloudera-scm /var/log/cloudera-scm-agent -R 配置 Cloudera Manager 仓库 对所有节点进行操作 12cp ~/soft/cm6.1.0/cloudera-manager.repo -P /etc/yum.repos.d/rpm --import https://archive.cloudera.com/cdh6/6.1.0/redhat7/yum/RPM-GPG-KEY-cloudera 安装 RPMS 对所有节点进行操作 12cd ~/soft/cm6.1.0/RPMSyum localinstall --nogpgcheck *.rpm 安装 CM Server 和 Agent master: 1yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server slave01,slave02: 1yum install cloudera-manager-daemons cloudera-manager-agent 5. 配置Cloudera Manager数据库（MySQL）安装MySQL服务12345wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmsudo rpm -ivh mysql-community-release-el7-5.noarch.rpmsudo yum updatesudo yum install mysql-serversudo systemctl start mysqld 配置和启动MySQL服务 如果MySQL服务正在运行的话，先将其停止：sudo systemctl stop mysqld； 删除（或者备份）日志文件/var/lib/mysql/ib_logfile0 and/var/lib/mysql/ib_logfile1； 修改/etc/my.cnf，Cloudrea 的推荐配置如下（具体详解参见官方说明）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socktransaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:symbolic-links = 0key_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space.#Replace &apos;/var/lib/mysql/mysql_binary_log&apos; with an appropriate path for your#system and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/mysql_binary_log#In later versions of MySQL, if you enable the binary log and do not set#a server_id, MySQL will not start. The server_id must be unique within#the replicating group.server_id=1binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidsql_mode=STRICT_ALL_TABLES 允许MySQL服务开机自启：sudo systemctl enable mysqld； 启动MySQL服务：sudo systemctl start mysqld； 修改密码：sudo /usr/bin/mysql_secure_installation； 123456789101112131415[...]Enter current password for root (enter for none):OK, successfully used password, moving on...[...]Set root password? [Y/n] YNew password:Re-enter new password:Remove anonymous users? [Y/n] Y[...]Disallow root login remotely? [Y/n] N[...]Remove test database and access to it [Y/n] Y[...]Reload privilege tables now? [Y/n] YAll done! 安装MySQL JDB Driver 所有节点 12345678910cd ~/softwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gztar zxvf mysql-connector-java-5.1.46.tar.gzmkdir -p /usr/share/java/cd mysql-connector-java-5.1.46cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar 为Cloudera 软件创建数据库 登录 mysql：mysql -u root -p； 为每个服务都创建一个数据库和用户： 123CREATE DATABASE &lt;database&gt; DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON &lt;database&gt;.* TO &apos;&lt;user&gt;&apos;@&apos;%&apos; IDENTIFIED BY &apos;&lt;password&gt;&apos;; Service Database User Cloudera Manager Server scm scm Activity Monitor amon amon Reports Manager rman rman Hue hue hue Hive Metastore Server metastore hive Sentry Server sentry sentry Cloudera Navigator Audit Server nav nav Cloudera Navigator Metadata Server navms navms Oozie oozie oozie 1234567891011121314151617181920212223242526272829303132333435# Cloudera Manager ServerCREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON scm.* TO &apos;scm&apos;@&apos;%&apos; IDENTIFIED BY &apos;scm000&apos;;# Activity MonitorCREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;amon000&apos;;# Reports ManagerCREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;rman000&apos;;# HueCREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue000&apos;;# Hive Metastore ServerCREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive000&apos;;# Sentry ServerCREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;sentry000&apos;;# Cloudera Navigator Audit ServerCREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON nav.* TO &apos;nav&apos;@&apos;%&apos; IDENTIFIED BY &apos;nav000&apos;;# Cloudera Navigator Metadata ServerCREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;navms000&apos;;# OozieCREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;oozie000&apos;; 检查刚才创建的数据库：SHOW DATABASES; 配置Cloudera Manager数据库运行修改scm_prepare_database.sh，更多配置参见： 1234567891011sudo /opt/cloudera/cm/schema/scm_prepare_database.sh &lt;databaseType&gt; &lt;databaseName&gt; &lt;databaseUser&gt;# 注意要输入密码，密码是刚才设置的，如：scm000sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scmsudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql amon amonsudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql rman rmansudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql hue huesudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql metastore hivesudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql sentry sentrysudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql nav navsudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql navms navmssudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql oozie oozie 6. 启动 Cloudrea Manger Server123systemctl start cloudera-scm-server# 查看日志tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 当你看到下图时，说明启动成功了 {% asset_img 启动成功.png %} 然后就可以在浏览器中输入：http://192.168.199.234:7180，看到下图便可以登陆了： Username: admin Password: admin 接收许可，选择免费版。 7. 集群配置添加节点登录网页版的管理系统之后，进入欢迎界面。接下来就可以添加主机了，也就是 slave 节点 添加存储库大体不变，直接继续 JDK安装因为之前已经装过了，所以不勾选。 提供 SSH 登录凭据 常见问题当前受管问题原因分析假如在安装的时候出现问题，如网络连接中断，机器死机，继续安装的时候可能会出现查询不到机器，并且根据ip搜索机器的时候，出现“当前受管”的状态为“是”，安装失败的机器不能再选择了。 问题解决方案当前受管 先停止所有服务。清除数据库。 删除Agent节点的UUID 123rm -rf /var/lib/cloudera-scm-agent/*# rm -rf /opt/cm-5.7.1/lib/cloudera-scm-agent/* 清空主节点CM数据库 进入主节点的Mysql数据库，然后drop database scm; 在主节点上重新初始化CM数据库 123456mysql -u root -pCREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON scm.* TO &apos;scm&apos;@&apos;%&apos; IDENTIFIED BY &apos;scm000&apos;;exit;sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm# /opt/cm-5.7.1/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -p123456 --scm-host localhost scm scm scm 集群密码：hadoop MySQL密码：mysql000 重启失败节点的agent服务删除agent目录下面的cm_guid文件，并重启失败节点的agent服务恢复。 1234567891011[root@esgyn001 ~]# cd /var/lib/cloudera-scm-agent/cm_guid response.avro uuid [root@esgyn001 cloudera-scm-agent]# rm -rf cm_guid [root@esgyn001 cloudera-scm-agent]# service cloudera-scm-agent restartcd /var/lib/cloudera-scm-agent/rm -rf cm_guid service cloudera-scm-agent restart OOZIE 无法创建表123456# oozie 无法创建表，只能把所有表全删了算了rm /dfs/nn/current/ -rfrm /dfs/snn/current/ -rfrm -rf /dfs/dn/current/rm -rf /var/lib/oozie/*]]></content>
      <categories>
        <category>Cloudera Manager</category>
      </categories>
      <tags>
        <tag>Cloudera Manager</tag>
        <tag>Hdoop</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GROBID 使用教程]]></title>
    <url>%2F2018%2F04%2FGrobid-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[简介GROBID是一个强大机器学习库，用于解析和抽取原始文档（如：PDF）的逻辑结构；是我在清华大学知识工程实验室主要研究和使用的工具；也是我本科毕业设计主要的研究内容。 官方文档地址：https://grobid.readthedocs.io/en/latest/Introduction/ 在此我简要介绍一下GROBID的功能及特点： 能够从PDF格式的文档中提取和解析出文献头部。这些提取出来的信息包含了文献详细信息，例如：标题、摘要、作者、单位、关键字等； 能够从PDF格式的文档中提取和解析出参考引用。支持从脚注里提取参考资料。这些信息在技术和科学文章中很少见，但在人文和社会科学的出版物中却很常见； 能够单独解析文章引用信息； 能够在专利出版物中提取专利和非专利参考文献； 能够解析出名称，尤其是标题中的作者姓名和引用中的作者姓名（两个不同的模型）； 能够解析隶属关系和地址块； 能够解析日期； 能够从PDF文章中提取全文，包括整个文档分段的模型和文本主体结构的模型。]]></content>
      <categories>
        <category>Grobid</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Grobid</tag>
        <tag>NLP</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
</search>
